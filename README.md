# QRNN
Quasi-Recurrent Neural Networks


The aim of this project was implementing a state-of-the-art architecture from a given scientific paper (see[1]). In this project we faced the so called Quasi-Recurrent Neural Networks (QRNNs),which face the limits of both Convolutional and Recurrent Neural Networks and try to solve it. There are mainly two different ways to process the data in deep learning:

    Process them in parallel
    Process them word-by-word

The first technique is often applied to images, the second one is used for text data, which need to be processed taking into account the context of the world in a sentence. With this paper has come to light that point 1 and point 2 above, can be combined mixing the main features of CNNs and RNNs in a sort of a new kind of Neural Networks called Quasi-Recurrent in which is possible to process all the text data in the same time like it happens for the images (what CNN actually does), taking context into account.
