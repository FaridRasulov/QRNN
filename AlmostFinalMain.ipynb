{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, embeddings, BATCH_SIZE, SEQ_LEN, train, beta=4e-6):\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.seq_len = SEQ_LEN\n",
    "        self.embeddings = embeddings\n",
    "        self.train = train\n",
    "\n",
    "        self.inputs = tf.placeholder(tf.int32, [BATCH_SIZE, SEQ_LEN], name=\"inputs\")\n",
    "        self.masks = tf.placeholder(tf.float32, [BATCH_SIZE, SEQ_LEN], name=\"mask\")\n",
    "        self.labels = tf.placeholder(tf.int32, [BATCH_SIZE], name=\"labels\")\n",
    "        self.train = tf.placeholder(tf.bool, [], name='train')\n",
    "\n",
    "        x, weights = self.forward()\n",
    "        loss = self.__call__(x)\n",
    "        \n",
    "        self.op = tf.train.RMSPropOptimizer(0.001).minimize(loss)\n",
    "        self.epoch = tf.Variable(0, dtype=tf.int32, trainable=False, name='epoch')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        masks = self.masks\n",
    "        labels = self.labels\n",
    "        outputs = tf.reduce_mean(x * tf.expand_dims(masks, -1), 1)\n",
    "        logits = tf.layers.dense(tf.squeeze(outputs), 2)\n",
    "        pred = tf.argmax(tf.nn.softmax(logits), -1)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "   \n",
    "        correct_prediction = tf.equal(tf.cast(pred, tf.int32), labels)\n",
    "        self.accuracy = tf.reduce_sum(tf.cast(correct_prediction,tf.float32)) / self.batch_size\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseModel(Model):\n",
    "    def forward(self):\n",
    "        inputs = self.inputs\n",
    "        num_layers = 1\n",
    "        input_size = 50\n",
    "        num_convs = 10\n",
    "        conv_size = 50\n",
    "        x = tf.expand_dims(tf.nn.embedding_lookup(self.embeddings, inputs), -1)\n",
    "        qrnn = DenseLayer(input_size,conv_size,num_convs,range(num_layers),num_layers, dropout=0.3)\n",
    "        x = qrnn(x, train=self.train)\n",
    "        weights = [l.W for l in qrnn.layers] + [l.b for l in qrnn.layers]\n",
    "        return tf.squeeze(x), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRNNLayer:\n",
    "    def __init__(self, input_size, conv_size, hidden_size, layer_id, pool='fo', zoneout=0.0, num_in_channels=1):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.conv_size = conv_size if conv_size%2==0 else conv_size+1\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer_id = layer_id\n",
    "        self.pool = pool\n",
    "        self.zoneout = zoneout\n",
    "        self.num_in_channels = num_in_channels\n",
    "        init = tf.random_normal_initializer()\n",
    "        filter_shape = [conv_size, input_size, num_in_channels, hidden_size*(len(pool)+1)]\n",
    "\n",
    "        with tf.variable_scope('QRNN/conv/'+str(layer_id)):\n",
    "            self.W = tf.get_variable('W', filter_shape, initializer=init, dtype=tf.float32)\n",
    "            self.b = tf.get_variable('b', [hidden_size*(len(pool)+1)], initializer=init, dtype=tf.float32)\n",
    "\n",
    "    def __call__(self, inputs, train=None):\n",
    "        gates = self.conv(inputs)\n",
    "        if self.zoneout and self.zoneout > 0.0:\n",
    "            F = gates[2]\n",
    "            F = tf.cond(train, lambda: 1-tf.nn.dropout(F, 1-self.zoneout), lambda: F)\n",
    "            gates[2] = F\n",
    "        if self.pool == 'f': return self.f_pool(gates)\n",
    "        elif self.pool == 'fo': return self.fo_pool(gates)\n",
    "        elif self.pool == 'ifo': return self.ifo_pool(gates)\n",
    "\n",
    "    def conv(self, inputs):\n",
    "        padded_inputs = tf.pad(inputs, [[0, 0], [self.conv_size - 1, 0], [0, 0], [0, 0]], \"CONSTANT\")\n",
    "        conv = tf.nn.conv2d(padded_inputs, self.W, strides=[1, 1, 1, 1],padding='VALID', name='conv'+str(self.layer_id))\n",
    "        conv += self.b\n",
    "        gates = tf.split(conv, (len(self.pool)+1), 3)\n",
    "        gates[0] = tf.tanh(gates[0])\n",
    "        for i in range(1, len(gates)):\n",
    "            gates[i] = tf.sigmoid(gates[i])\n",
    "        return gates\n",
    "\n",
    "    def unstack(self, gates, pooling):\n",
    "        if pooling == 'f': Z, F = gates\n",
    "        elif pooling == 'fo': Z, F, O = gates\n",
    "        elif pooling == 'ifo': Z, F, O, I = gates\n",
    "\n",
    "        Z = tf.unstack(Z, axis=1)\n",
    "        F = tf.unstack(F, axis=1)\n",
    "        if pooling == 'f': return Z,F\n",
    "        O = tf.unstack(O, axis=1)\n",
    "        if pooling == 'fo': return Z,F,O\n",
    "        I = tf.unstack(O, axis=1)\n",
    "        if pooling == 'ifo': return Z,F,O,I\n",
    "\n",
    "    def fo_pool(self, gates):\n",
    "        Z, F, O = self.unstack(gates,'fo')\n",
    "        C = [tf.fill(tf.shape(Z[0]), 0.0)] #tf.zeros(tf.shape(Z[0]), tf.float32) #\n",
    "        H = []\n",
    "        for i in range(len(Z)):\n",
    "            c = tf.multiply(F[i], C[-1]) + tf.multiply(1-F[i], Z[i])\n",
    "            h = tf.multiply(O[i], c)\n",
    "            C.append(c)\n",
    "            H.append(h)\n",
    "        H = tf.stack(H, axis=1)\n",
    "        return tf.transpose(H, perm=[0, 1, 3, 2])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_size, conv_size, hidden_size, layer_ids, num_layers, zoneout=0.0, dropout=0.0):\n",
    "        self.layers = []\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.layers = [QRNNLayer(hidden_size, conv_size, hidden_size,layer_ids[i], pool='fo', zoneout=zoneout, num_in_channels=i+1) for i in range(num_layers)]\n",
    "\n",
    "    def __call__(self, inputs, train=None):\n",
    "        inputs = tf.layers.dense(tf.transpose(inputs, [0, 1, 3, 2]), self.hidden_size)\n",
    "        inputs = tf.transpose(inputs, [0, 1, 3, 2])\n",
    "        for layer in self.layers:\n",
    "            outputs =  layer(inputs, train=train)\n",
    "            if self.dropout and self.dropout > 0:\n",
    "                keep_prob = 1 - self.dropout\n",
    "                outputs = tf.cond(train, lambda: tf.nn.dropout(outputs, keep_prob), lambda: outputs)\n",
    "            inputs = tf.concat([inputs, outputs], 3)\n",
    "        return tf.squeeze(outputs[:, :, :, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_restore_parameters(sess, saver, checkpoint_path):\n",
    "    try:\n",
    "        os.mkdir(checkpoint_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        print (\"Loading parameters\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print (\"Initializing fresh parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(vocab, path, dim = 300):\n",
    "    vocab_dict = {word: int(_id) for _id, word in vocab.items()}\n",
    "    embed_id = path.split('.')[-2]\n",
    "    if embed_id+'_imdb.json' not in os.listdir('.'):\n",
    "        embeds = {}\n",
    "        with open(path, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                split = line.split()\n",
    "                word = split[0].encode('utf-8').lower()\n",
    "                vec = split[1:]\n",
    "                if word in vocab_dict.keys():\n",
    "                    embeds[vocab_dict[word]] = list(map(float,vec))\n",
    "        with open(embed_id+'_imdb.json', 'w') as f:\n",
    "            f.write(json.dumps(embeds))\n",
    "    else:\n",
    "        with open(embed_id+'_imdb.json') as f:\n",
    "            embeds = {int(_id): word for _id, word in json.loads(f.read()).items()}\n",
    "    embed_list = []\n",
    "    for i in range(3):\n",
    "        var = tf.Variable(tf.contrib.layers.xavier_initializer()(shape=[dim]), dtype=tf.float32)\n",
    "        embed_list.append(var)\n",
    "    for _id, word in vocab.items():\n",
    "        if int(_id) in embeds.keys(): embed_list.append(tf.constant(embeds[_id], dtype=tf.float32))\n",
    "        else: embed_list.append(embed_list[2])\n",
    "    return tf.stack(embed_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imdbDataset(Dataset):\n",
    "    def __init__(self, dataset, seq_len=100):\n",
    "        self.x = dataset[0]\n",
    "        self.pad_inputs(seq_len)\n",
    "        self.get_masks(seq_len)\n",
    "        self.y = dataset[1]\n",
    "\n",
    "    def pad_inputs(self, seq_len):\n",
    "        new_xs = []\n",
    "        for x in self.x:\n",
    "            if len(x) > seq_len: x = x[:seq_len]\n",
    "            elif len(x) < seq_len: x += [0] * (seq_len - len(x))\n",
    "            assert len(x) == seq_len\n",
    "            new_xs.append(x)\n",
    "        self.x = new_xs\n",
    "\n",
    "    def get_masks(self, seq_len):\n",
    "        self.masks = []\n",
    "        for x in self.x:\n",
    "            mask = [1.0 * (x_i != 0) for x_i in x]\n",
    "            assert len(mask) == seq_len\n",
    "            self.masks.append(mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.masks[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(batch_size=100, num_words=1000, seq_len=100):\n",
    "    train, test = imdb.load_data(num_words=num_words)\n",
    "    vocab = imdb.get_word_index()\n",
    "    vocab = {int(_id): word.encode('utf-8').lower() for word, _id in vocab.items() if _id <= num_words}\n",
    "    train = imdbDataset(train, seq_len=seq_len)\n",
    "    test = imdbDataset((test[0], test[1]), seq_len=seq_len)\n",
    "    return (DataLoader(train, batch_size), DataLoader(test, batch_size),vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, sess, dataset, train=False):\n",
    "    avg_correct = 0.0\n",
    "    for x, masks, y in dataset:\n",
    "        input_feed = {\n",
    "            model.inputs: np.array([i.numpy() for i in x]).T,\n",
    "            model.masks: np.array([i.numpy() for i in masks]).T,\n",
    "            model.labels: np.array([i.numpy() for i in y]).T,\n",
    "            model.train: train\n",
    "        }\n",
    "\n",
    "        if train:\n",
    "            output_feed = [model.accuracy, model.op]\n",
    "            num_correct, _ = sess.run(output_feed, input_feed)\n",
    "        else:\n",
    "            output_feed = [model.accuracy]\n",
    "            num_correct, = sess.run(output_feed, input_feed)\n",
    "            \n",
    "        avg_correct += num_correct\n",
    "    avg_correct /= len(dataset)\n",
    "    return avg_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Farid\\Anaconda33\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 500\n",
    "SEQ_LEN = 500\n",
    "VOCAB_SIZE = 500\n",
    "\n",
    "train_data, test_data, vocab = get_datasets(BATCH_SIZE,VOCAB_SIZE,SEQ_LEN)\n",
    "embeddings = get_embeddings(vocab, './glove.6B/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-33e62ec14b94>:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-33e62ec14b94>:16: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Farid\\Anaconda33\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Initializing fresh parameters\n",
      "test acc: 0.49072002053260805\n"
     ]
    }
   ],
   "source": [
    "model = DenseModel(embeddings, BATCH_SIZE, SEQ_LEN, VOCAB_SIZE)\n",
    "saver = tf.train.Saver()\n",
    "train = True\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ckpt_path = os.path.join('checkpoints', 'dense')\n",
    "    check_restore_parameters(sess, saver, ckpt_path)\n",
    "    \n",
    "    if train:\n",
    "        epoch = model.epoch.eval()\n",
    "        for i in range(epoch, NUM_EPOCHS):\n",
    "            train_acc = run(model, sess, train_data, train=True)\n",
    "            print ('epoch', i, 'train acc:', train_acc)\n",
    "            sess.run(tf.assign(model.epoch, i+1))\n",
    "        saver.save(sess, ckpt_path)\n",
    "    \n",
    "    test_acc = run(model, sess, test_data)\n",
    "    print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
